{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm # Gives us a cool progress bar when training :)\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Hyper parameters\n",
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 1\n",
    "LOG_RATE = 10\n",
    "\n",
    "# Displaying our dataset\n",
    "NUM_IMGS_TO_SHOW = BATCH_SIZE\n",
    "\n",
    "MODEL_SAVE_PATH = './cifar_net.pth'\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "# CPU vs GPU setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can choose your gpus here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Currently downloading but should leave it a smaller set on the repo instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transform Docs](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "Normalize(means, stds) where means and stds are tuples with each element corresponding to a channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the dataset\n",
    "It's a good idea to have a look at your data to see if everything seems good.\n",
    "\n",
    "In PyTorch iamges are represented 'channel-first' i.e. (Channels, Height, Width) or CHW\n",
    "\n",
    "On the other hand, Tensorflow uses 'channel-last' i.e. (Height, Width, Channels) HWC\n",
    "\n",
    "pyplot imshow expects channel-last therefore we need to transpose our image from CHW to HWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(img.shape)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(images[0])\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images[NUM_IMGS_TO_SHOW]))\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(NUM_IMGS_TO_SHOW)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our CNN model\n",
    "[Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d) (in_channels, out_channels, kernel_size)\n",
    "\n",
    "Remember: out_channels = number of filters as by definition to be able to carry out the dot product \n",
    "\n",
    "[MaxPool2d](https://pytorch.org/docs/stable/nn.html#maxpool2d) (kernel_size, stride)\n",
    "\n",
    "[Linear](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "(in_features, out_features)\n",
    "\n",
    "\n",
    "Note that x.view reshapes our tensor. In this example we want to reshape the output of our 2nd max pooling (after our conv2 layer) which has an output shape of (n, 16, 5, 5) into (1, 16*5*5) = (n, 400) as the fully connected 'linear' layer expects a vector shape. Where 'n' is our batch_size.\n",
    "\n",
    "The -1 means to automatically fill the remaining dimension which in this case would get filled to 'n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining our CNN Model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model summary if we feed in an RGB image of size (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             456\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Linear-5                  [-1, 120]          48,120\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (3, 32, 32), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import and finetune a readymade model. For example the ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Getting resnet18 from the torchvision model zoo\n",
    "resnet18 = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "# Just 'feature extracting'\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(resnet18, (3, 32, 32), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.537251568\n",
      "[1,    20] loss: 2.419472909\n",
      "[1,    30] loss: 2.286239076\n",
      "[1,    40] loss: 2.207667828\n",
      "[1,    50] loss: 2.128077030\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 24 %\n"
     ]
    }
   ],
   "source": [
    "test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % LOG_RATE == LOG_RATE - 1:    # print every LOG_RATE mini-batches\n",
    "                print('[%d, %5d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / LOG_RATE))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.304206991\n",
      "[1,    20] loss: 2.304518533\n",
      "[1,    30] loss: 2.302389884\n",
      "[1,    40] loss: 2.304218841\n",
      "[1,    50] loss: 2.302620554\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our predictions using the our 'net' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = net(images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # total accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
